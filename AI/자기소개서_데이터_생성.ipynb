{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSyrNEJKMcMzzy2TSA2RID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yeyeong99/DevJS/blob/main/AI/%EC%9E%90%EA%B8%B0%EC%86%8C%EA%B0%9C%EC%84%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnAwYP9kuL3j",
        "outputId": "6328f3a2-3b7b-4cf7-e565-b0f6515a4a81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3zj_peEOuK31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import groq\n",
        "import random\n",
        "import re\n",
        "# 키를 여기에 넣어주세요\n",
        "GROQ_API_KEY=\"\"\n",
        "\n",
        "# Groq 클라이언트 초기화\n",
        "client = groq.Groq(\n",
        "    api_key=GROQ_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_korean(text):\n",
        "    \"\"\"응답에서 한국어 비율이 낮으면 다시 요청하도록 설정\"\"\"\n",
        "    korean_ratio = len(re.findall(r\"[가-힣]\", text)) / len(text) if text else 0\n",
        "    contains_hanja = bool(re.search(r\"[\\u4E00-\\u9FFF]\", text))  # 한자 포함 여부 확인\n",
        "\n",
        "\n",
        "    if korean_ratio < 0.5 or contains_hanja:\n",
        "        return None  # 한국어 비율이 낮거나 한자가 포함되면 None 반환\n",
        "    return text"
      ],
      "metadata": {
        "id": "tw3OdWF7G7Lj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_jd():\n",
        "    \"\"\"Groq의 Llama-3을 이용해 IT 스킬 생성\"\"\"\n",
        "    prompt = \"\"\"당신은 IT 직무를 담당하는 채용담당자입니다.\n",
        "    IT 직무와 관련해 자격 요건을 한 가지만 한국어로만!!!! 작성해주세요.\n",
        "    예를 들어 다음과 같은 요건이 있을 수 있습니다.\n",
        "\n",
        "    Python을 활용한 프로그래밍 능력\n",
        "    SQL 활용 능력\n",
        "    타부서와의 협업 능력\n",
        "    AI 활용 전략 수립 및 운영/관리\n",
        "    머신러닝 및 알고리즘에 관심을 가지고 해당 분야의 역량을 보유하신 분\n",
        "    Auto/Fraud 관련 데이터 분석 및 예방 전략 수립\n",
        "    새로운 시도를 즐기고, IT를 통해 세상의 변화를 이끌어가고 싶은 분\n",
        "    금융, 제조, 물류 등 주요 사업 분야의 데이터를 분석 및 제공하여 고객의 효율적인 자원 관리를 위한 인사이트를 제공\n",
        "    SAP 기반 Erp 업무 시스템 개발 및 운영\n",
        "    식품/유통/물류/그룹통합 시스템 구축 및 운영\n",
        "\n",
        "    위의 예시와 동일하지 않게, 최대한 다양한 예시를 떠올려 한 가지만!!!! 반환해주세요. 요청한 자격 요건 외에 Here's~, 여기에 등등 어떤 데이터를 제공하겠다는 안내 문구는 쓰지 마세요.\n",
        "    그냥 요청한 데이터만 주세요. / 외의 특수문자도 포함하지 마세요.\n",
        "    반드시 한국어 써주세요. Python, Java와 같이 프로그래밍 언어 고유 명사를 영어로 쓰는 것만 가능합니다.\n",
        "    한국어 이외의 언어를 포함하지 마세요.\n",
        "    sở유와 같은 표현 쓰지 마세요. 반드시 한자를 제외하세요. 반드시 영어도 제외하세요.\n",
        "    Here is a requirement in Korean: 같이 안내 문구 포함하지 마세요.\n",
        "    반드시 한국어로만 작성하세요.\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "\n",
        "            return result  # 한국어가 충분하면 반환\n"
      ],
      "metadata": {
        "id": "abMyN6zCxx1c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_motivation_essay(jd):\n",
        "    \"\"\"Groq의 Llama-3을 이용해 IT 동기부여 생성\"\"\"\n",
        "    prompt = f\"\"\"당신은 IT 직무에 지원하는 지원자입니다.\n",
        "    {jd}를 반영해 자신의 동기 부여를 담은 자기소개서를 한국어로!!!!!! 한 문단!!!!으로 작성해주세요.\n",
        "    이때 반영 정도는 10점 만점으로 했을 때 {random.randint(0, 10)}정도로 해주세요.\n",
        "    작성된 자기소개서 외에 추론 과정, Here's~, 여기에 등등 어떤 데이터를 제공하겠다는 안내 문구는 쓰지 마세요.\n",
        "    그냥 요청한 데이터만 주세요. \\n도 포함하지 마세요. / 외의 특수문자도 포함하지 마세요.\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "\n",
        "            return result  # 한국어가 충분하면 반환\n"
      ],
      "metadata": {
        "id": "JPUDCjlSwQBN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_relevant_experience_essay(jd):\n",
        "    \"\"\"Groq의 Llama-3을 이용해 IT 관련 경험 생성\"\"\"\n",
        "    prompt = f\"\"\"당신은 IT 직무에 지원하는 지원자입니다.\n",
        "    자신의 인턴십, 프로젝트 등 지원 직무와 관련된 경험을 담은 자기소개서를 한국어로!!!!! 한 문단으로 작성해주세요.\n",
        "    이때 자신의 역할과 수행한 업무를 구체적으로 작성해주세요.\n",
        "    또한 {jd}를 반영하는데, 반영 정도는 10점 만점으로 했을 때 {random.randint(0, 10)}정도로 해주세요.\n",
        "    작성된 자기소개서 외에 안내문구, 추론 과정은 따로 반환하지 마세요. \\n도 포함하지 마세요.\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "            return result  # 한국어가 충분하면 반환"
      ],
      "metadata": {
        "id": "v5TS-IuExN4R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_strength_essay(jd):\n",
        "    \"\"\"Groq의 Llama-3을 이용해 IT 강점 생성\"\"\"\n",
        "    prompt = f\"\"\"당신은 IT 직무에 지원하는 지원자입니다.\n",
        "    {jd}를 반영해 자신의 강점을 담은 자기소개서를 한국어로!!!! 한 문단으로 작성해주세요.\n",
        "    이때 반영 정도는 10점 만점으로 했을 때 {random.randint(0, 10)}정도로 해주세요.\n",
        "    작성된 자기소개서 외에 추론 과정, Here's~, 여기에 등등 어떤 데이터를 제공하겠다는 안내 문구는 쓰지 마세요.\n",
        "    그냥 요청한 데이터만 주세요. 추론 과정은 따로 반환하지 마세요. \\n도 포함하지 마세요.  / 외의 특수문자도 포함하지 마세요.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "            return result  # 한국어가 충분하면 반환"
      ],
      "metadata": {
        "id": "LgJJtktKxgqf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_technical_skills_essay(jd):\n",
        "    \"\"\"Groq의 Llama-3을 이용해 IT 스킬 생성\"\"\"\n",
        "    prompt = f\"\"\"당신은 IT 직무에 지원하는 지원자입니다.\n",
        "    {jd}를 반영해 자신의 테크니컬 스킬을 담은 자기소개서를 한국어로, 한 문단으로 작성해주세요.\n",
        "    이때 반영 정도는 10점 만점으로 했을 때 {random.randint(0, 10)}정도로 해주세요.\n",
        "    작성된 자기소개서 외에 추론 과정, Here's~, 여기에 등등 어떤 데이터를 제공하겠다는 안내 문구는 쓰지 마세요.\n",
        "    그냥 요청한 데이터만 주세요. 마지막에\\n, \\n\\n\\n\\n도 포함하지 마세요.  / 외의 특수문자도 포함하지 마세요.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "\n",
        "            return result  # 한국어가 충분하면 반환"
      ],
      "metadata": {
        "id": "2LTlxgkvxHke"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_essay(jd, essay):\n",
        "    \"\"\"Groq의 Llama-3을 이용해 자기소개서 피드백 제공\"\"\"\n",
        "    prompt = f\"\"\"당신은 IT 직무 전문 컨설턴트로, 다음 자기소개서가 jd를 잘 반영하고 있는지 여부를 파악해야합니다.\n",
        "    - 자기소개서:\\n\\n{essay}\\n\\n\n",
        "    - jd: {jd}\n",
        "    다음 내용을 포함한 첨삭 내용을 한국어로 작성해주세요.\n",
        "    작성된 자기소개서 외에 안내문구, 추론 과정은 따로 반환하지 마세요.\n",
        "    1. 피드백의 첫 문장은 항상 JD가 잘 반영되었는지 여부를 자신이 기업에 기여할 수 있는지, 그러한 주장에 대한 구체적인 근거나 사례가 있는지, 주장만 있지는 않은지, 너무 일반적이라 경쟁력 없는 말이진 않은지를 기준으로 10점 만점에 몇 점인지 알려주는 문장으로 시작. 이때 점수는 0~10 사이의 정수로 제한함.\n",
        "    2. 1. 조건의 근거가 되는 문장이 어느 부분인지 짚어주고,\n",
        "    3. 장점에 대해 언급, 3번에 비해 비중을 작게 두세요.\n",
        "    4. 부족한 부분이 있는 문장을 짚어주기, 2번에 비해 3번에 비중을 더 크게 두세요.\n",
        "    5. 3번에 대해 보완된 예시 문장\n",
        "\n",
        "    작성된 자기소개서 외에 추론 과정, Here's~, 여기에 등등 어떤 데이터를 제공하겠다는 안내 문구는 쓰지 마세요.\n",
        "    최대한 공손하게 존댓말로, 일관성 있는 기준으로 작성해주세요.\n",
        "    그냥 요청한 데이터만 주세요. 특수문자도 포함하지 말고 그냥 쭉 이어진 한 문단으로 제시해주세요.\n",
        "\n",
        "    다음은 위의 요청 사항이 모두 반영된 예시입니다. 형식만 참고 해주세요.\n",
        "    JD가 자기소개서에 반영된 여부를 10점 만점으로 표현했을 때, 점수는 6점입니다.\n",
        "    'AI 활용 전략 수립 및 운영/관리'라는 부분이 자기소개서에서 명확하게 반영되지 않았기 때문입니다.\n",
        "    지원자는 AI 기술에 대한 관심과 경험을 언급하지만, 전략적인 활용이나 운영/관리와 관련된 구체적인 사례가 부족합니다.\n",
        "    특히, 'AI 모델을 활용해 다양한 분야에서 위험을 감소시키는 리스크팀에 관심'이 있지만, AI 전략 수립이나 관리의 경험이 구체적으로 드러나지 않았습니다.\n",
        "    이를 보완하기 위해선 '자체 개발한 AI 모델을 활용하여 리스크 분석을 개선하고, 이를 바탕으로 AI 전략을 수립하고 운영한 경험'을 강조하는 표현을 추가하는 것이 좋을 것 같습니다.\n",
        "    이렇게 한다면 AI 활용 전략 수립 및 운영/관리 역량이 더욱 강조될 것입니다.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Gemma2-9b-It\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"반드시 모든 응답을 한국어로 작성하세요.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "\n",
        "        # ensure_korean 함수로 한국어 비율 확인\n",
        "        if ensure_korean(result):\n",
        "            result = result.replace(\"\\n\", \"\")\n",
        "\n",
        "            return result  # 한국어가 충분하면 반환"
      ],
      "metadata": {
        "id": "mx4cRgKluRwn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 생성 예시"
      ],
      "metadata": {
        "id": "1g_OdhjB3-m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jd = generate_jd()"
      ],
      "metadata": {
        "id": "_U_3ngjAz3X0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "motivation = generate_motivation_essay(jd)"
      ],
      "metadata": {
        "id": "x2kSySgaz6-u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "motivation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qb4pAaGV0ckC",
        "outputId": "06afdbb0-b981-4f5c-c91f-084299cc845f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'능동적으로 문제를 해결하고 분석하려는 꾸준한 습관을 통해 데이터 시각화 도구에 대해 6점 정도의 이해를 갖게 되었습니다. 다양한 데이터를 효과적으로 시각화하여 핵심 정보를 명확하게 전달하고 이해도를 높이는 툴의 중요성을 인지했고, 이에 대한 수련을 통해 비즈니스 문제 해결에 더욱 실질적인 도움을 줄 수 있는 전문가로 성장하고 싶습니다. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_relevant_experience_essay(jd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "qSQt10yQ0dPA",
        "outputId": "4e502797-47e9-4483-ff00-5e9db670dab1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'지난 학업 근무 기간 동안 데이터 분석에 대한 관심을 갖게 되어 다수의 프로젝트에 참여했습니다. 특히, 서울지역 택시 지역별 주요 지출 현황 분석 프로젝트에서 데이터 수집 및 정제를 역할을 수행하며, Tableau를 활용하여 시각화된 결과물을 통해 뚜렷한 경향을 발견하고 보고서로 정리했습니다. 이를 통해 데이터 분석의 전 과정을 체험하고, 데이터 시각화 도구를 활용하여 이해하기 쉽고 효과적인 정보 전달 방법을 숙달해왔습니다 (점수: 9/10). '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_strength_essay(jd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "z0CSlCl92a9k",
        "outputId": "83baaab5-ce2b-4465-9388-56bf988f5b9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jqjv8v98e5nb1w05f9brc89a` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500167, Requested 180. Please try again in 1m0.0884s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4c224f1e8a45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_strength_essay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-7bd701ed98d0>\u001b[0m in \u001b[0;36mgenerate_strength_essay\u001b[0;34m(jd)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gemma2-9b-It\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jqjv8v98e5nb1w05f9brc89a` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 500167, Requested 180. Please try again in 1m0.0884s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_technical_skills_essay(jd)"
      ],
      "metadata": {
        "id": "nzIB-d-32fzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_essay(jd, motivation)"
      ],
      "metadata": {
        "id": "9FqOeyda2qmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5000개 시범 생성"
      ],
      "metadata": {
        "id": "pvlbogAD4Aal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "X2EiNTKH4g76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 생성 및 저장\n",
        "motivation_samples = []\n",
        "relevant_experience_samples = []\n",
        "strength_samples = []\n",
        "technical_skills_samples = []\n",
        "\n",
        "for _ in range(5000):  # 원하는 개수만큼 반복\n",
        "    jd = generate_jd()\n",
        "    print(f\"{_ + 1}: {jd}\")\n",
        "    # 동기 부여\n",
        "    motivation = generate_motivation_essay(jd)\n",
        "    feedback = evaluate_essay(jd, motivation)\n",
        "\n",
        "    motivation_samples.append({\n",
        "    'Highlight': jd,\n",
        "    \"SelfIntroduction\": motivation,\n",
        "    \"Feedback\": feedback\n",
        "    })\n",
        "    # 관련 경험\n",
        "    relevant_experience = generate_relevant_experience_essay(jd)\n",
        "    feedback = evaluate_essay(jd, relevant_experience)\n",
        "\n",
        "    relevant_experience_samples.append({\n",
        "        'Highlight': jd,\n",
        "        \"SelfIntroduction\": relevant_experience,\n",
        "        \"Feedback\": feedback\n",
        "    })\n",
        "    # 강점\n",
        "    strength = generate_strength_essay(jd)\n",
        "    feedback = evaluate_essay(jd, strength)\n",
        "\n",
        "    strength_samples.append({\n",
        "        'Highlight': jd,\n",
        "        \"SelfIntroduction\": strength,\n",
        "        \"Feedback\": feedback\n",
        "    })\n",
        "\n",
        "    # 태크 스킬\n",
        "    technical_skills = generate_technical_skills_essay(jd)\n",
        "    feedback = evaluate_essay(jd, technical_skills)\n",
        "\n",
        "    technical_skills_samples.append({\n",
        "        'Highlight': jd,\n",
        "        \"SelfIntroduction\": technical_skills,\n",
        "        \"Feedback\": feedback\n",
        "    })"
      ],
      "metadata": {
        "id": "P-Oi_Q5DC06y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON 파일로 저장\n",
        "\n",
        "with open(\"it_resume_data_groq.json\", \"a\", encoding=\"utf-8\") as f:\n",
        "    json.dump(motivation_samples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"it_resume_data_groq.json\", \"a\", encoding=\"utf-8\") as f:\n",
        "    json.dump(strength_samples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"it_resume_data_groq.json\", \"a\", encoding=\"utf-8\") as f:\n",
        "    json.dump(relevant_experience_samples, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"it_resume_data_groq.json\", \"a\", encoding=\"utf-8\") as f:\n",
        "    json.dump(technical_skills_samples, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "WjVUldD-C4wJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}